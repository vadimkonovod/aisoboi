{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Строим инвертированный индекс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from collections import OrderedDict\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def readline(corpus):\n",
    "    line = corpus.readline()\n",
    "    return line\n",
    "\n",
    "def process_line(line):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    signs = punctuation + ' '\n",
    "    # разбиваем на токены\n",
    "    # долбавляем если не стоп-слово и не знак пунктуации\n",
    "    words = [stemmer.stem(w) for w in word_tokenize(line) if (w and w not in stop_words and w not in signs)]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def index_by_field(data, field):  \n",
    "    unsorted_index = {}\n",
    "    for item in data:\n",
    "        i = item['I']\n",
    "        document = item[field]\n",
    "        document_index = {}\n",
    "        # посчитаем частоту слов в документе\n",
    "        for word in document.split(' '):\n",
    "            if word:\n",
    "                document_index[word] = document_index[word] + 1 if word in document_index else 1\n",
    "        # добавим это к общему индексу\n",
    "        for word in document_index:\n",
    "            if word in unsorted_index:\n",
    "                unsorted_index[word].append((i, document_index[word])) \n",
    "            else:\n",
    "                unsorted_index[word] = [(i, document_index[word])]\n",
    "    \n",
    "    # отсортируем индекс по словам\n",
    "#     sorted_index = OrderedDict(sorted(unsorted_index.items()))\n",
    "    return unsorted_index\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus_data = []\n",
    "with open('data/cran.all.1400') as corpus:\n",
    "    line = readline(corpus)\n",
    "    i = 0\n",
    "    \n",
    "    while line:\n",
    "        if line.startswith('.I'):\n",
    "            i = int(line.split()[1])\n",
    "            item = {\n",
    "                'I': i - 1,\n",
    "                'T': '',\n",
    "                'W': ''\n",
    "            }\n",
    "        \n",
    "        if line.startswith('.T'):\n",
    "            line = readline(corpus)\n",
    "            # Склеиваем title в одну строку\n",
    "            while line and not line.startswith('.A'):\n",
    "                item['T'] += (' ' + process_line(line.strip())).strip()\n",
    "                line = readline(corpus)      \n",
    "        elif line and line.startswith('.W'):\n",
    "            line = readline(corpus)\n",
    "            # Склеиваем abstract в одну строку\n",
    "            while line and not line.startswith('.I'):\n",
    "                item['W'] += ' ' + process_line(line.strip())\n",
    "                line = readline(corpus)\n",
    "            corpus_data.append(item)\n",
    "        else:     \n",
    "            line = readline(corpus)\n",
    "\n",
    "# индекс по полю 'W'\n",
    "abstract_index = index_by_field(corpus_data, 'W')\n",
    "\n",
    "# индекс по полю 'T'\n",
    "title_index = index_by_field(corpus_data, 'T')\n",
    "\n",
    "# посчитаем среднюю длину документов в индексе\n",
    "abstract_L = reduce(lambda s, d: s + len(d['W'].split(' ')), corpus_data, 0) / float(len(corpus_data))\n",
    "title_L = reduce(lambda s, d: s + len(d['T'].split(' ')), corpus_data, 0) / float(len(corpus_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Реализуем алгоритм поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from itertools import groupby\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "index = abstract_index\n",
    "average_L = abstract_L\n",
    "\n",
    "def merge_lists(l1, l2):\n",
    "    j = i =  0\n",
    "    result = []\n",
    "    while i < len(l1) and j < len(l2):\n",
    "        if l1[i][1] == l2[j][1]:\n",
    "            result.append(l1[i])\n",
    "            result.append(l2[j])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif l1[i][1] < l2[j][1]:\n",
    "            result.append(l1[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(l2[j])\n",
    "            j += 1\n",
    "    if i < len(l1):\n",
    "        result.extend(l1[i:])\n",
    "    if j < len(l2):\n",
    "        result.extend(l2[j:])\n",
    "    return result\n",
    "\n",
    "def tfd(f_t_d, L_d, k, b):\n",
    "    return f_t_d * (k + 1) / (k * ((1 - b) + b * L_d / average_L) + f_t_d)\n",
    "\n",
    "def tfq(f_t_q, k):\n",
    "    return f_t_q * (k + 1) / (k + f_t_q)\n",
    "\n",
    "def idf(N, N_t):\n",
    "    return log(1 + (N - N_t + 0.5) / (N_t + 0.5))\n",
    "\n",
    "    \n",
    "def search(n_query, field, k=1.2, b=0.75, k2=1):\n",
    "    rsv_threshold = 20\n",
    "    full_postings_list = []\n",
    "    query_f = defaultdict(int)\n",
    "    # по каждому слову из query ищем в индексе список документов\n",
    "    # и сохраняем в следующем виде: [(<token>, <d_id>, <token_frequency>), ...]\n",
    "    for token in n_query.split(' '):\n",
    "        if token in index:\n",
    "            token_list = map(lambda t: (token, t[0], t[1]), index[token])\n",
    "            full_postings_list.extend(token_list)\n",
    "            query_f[token] += 1\n",
    "    docs_rsv = defaultdict(int)\n",
    "    idf_sum = defaultdict(int)\n",
    "    # Считаем rsv для всех документов\n",
    "    for d in full_postings_list:\n",
    "        idf_t = idf(len(index), len(index[d[0]]))\n",
    "        tf_d = tfd(d[2], len(corpus_data[d[1]][field].split(' ')), k, b)\n",
    "        tf_q = tfq(query_f[d[0]], 1)\n",
    "        docs_rsv[d[1]] += idf_t * tf_d * tf_q\n",
    "        idf_sum[d[1]] += tf_d\n",
    "    normilized_docs_rsv = {d_id: docs_rsv[d_id] / idf_sum[d_id] for d_id in docs_rsv}\n",
    "    \n",
    "    top_docs = { d: docs_rsv[d] for d in docs_rsv if docs_rsv[d] > rsv_threshold}\n",
    "    if len(top_docs) > 0:\n",
    "        top_docs_ids = sorted(top_docs, key=lambda k: top_docs[k], reverse=True)\n",
    "    else:\n",
    "        top_docs_ids = sorted(docs_rsv, key=lambda k: docs_rsv[k], reverse=True)[:10]\n",
    "        \n",
    "#     top_docs_ids = sorted(docs_rsv, key=lambda k: docs_rsv[k], reverse=True)[:10]\n",
    "\n",
    "#     top_docs = sorted(normilized_docs_rsv, key=lambda k: normilized_docs_rsv[k], reverse=True)[:10]\n",
    "#     for d in top_docs_ids:\n",
    "#         print docs_rsv[d]\n",
    "    return top_docs_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. Оценка качества поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_measure():\n",
    "    groundtruth_file = 'data/qrel_clean'\n",
    "    answer_file = 'data/answer'\n",
    "\n",
    "    q2reld = {}\n",
    "    for line in open(groundtruth_file):\n",
    "        qid, did = [int(x) for x in line.split()]\n",
    "        if qid in q2reld.keys():\n",
    "            q2reld[qid].add(did)\n",
    "        else:\n",
    "            q2reld[qid] = set([did])\n",
    "\n",
    "    q2retrd = {}\n",
    "    for line in open(answer_file):\n",
    "        qid, did = [int(x) for x in line.split()]\n",
    "        if qid in q2retrd.keys():\n",
    "            q2retrd[qid].append(did)\n",
    "        else:\n",
    "            q2retrd[qid] = [did]\n",
    "\n",
    "    N = len(q2retrd.keys())\n",
    "    precision = sum([len(q2reld[q].intersection(q2retrd[q]))*1.0/len(q2retrd[q]) for q in q2retrd.keys()]) / N\n",
    "    recall = sum([len(q2reld[q].intersection(q2retrd[q]))*1.0/len(q2reld[q]) for q in q2retrd.keys()]) / N\n",
    "    return (2*precision*recall/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33010851261053914"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_search(k=1.32, b=0.95, k2=1):\n",
    "    queries = []\n",
    "    with open('data/cran.qry') as test_queries:\n",
    "        line = readline(test_queries)\n",
    "        i = 0\n",
    "\n",
    "        while line:\n",
    "            if line.startswith('.I'):\n",
    "                i = int(line.split()[1])\n",
    "                query = {\n",
    "                    'I': i,\n",
    "                    'W': ''\n",
    "                }\n",
    "                line = readline(test_queries)\n",
    "\n",
    "            if line and line.startswith('.W'):\n",
    "                line = readline(test_queries)\n",
    "                while line and not line.startswith('.I'):\n",
    "                    query['W'] += (' ' + process_line(line)).strip()\n",
    "                    line = readline(test_queries)\n",
    "                queries.append(query)\n",
    "\n",
    "    with open('data/answer', 'w') as answer:\n",
    "        for i, q in enumerate(queries):\n",
    "#             for d in search(q['W'], 'T', k, b, k2):  \n",
    "            for d in search(q['W'], 'W', k, b, k2):            \n",
    "                answer.write('%d %d\\n'%(i + 1, d + 1))\n",
    "                \n",
    "test_search()\n",
    "f_measure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
